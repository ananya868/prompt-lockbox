{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d903b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91790\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to import: 5.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91790\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\91790\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\91790\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\91790\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load model and tokenizer: 4.27 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "s = time.time()\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "e = time.time()\n",
    "print(f\"Time taken to import: {e - s:.2f} seconds\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "f = time.time()\n",
    "model_name = \"naver/splade-cocondenser-ensembledistil\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "g = time.time()\n",
    "print(f\"Time taken to load model and tokenizer: {g - f:.2f} seconds\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def splade_encode(texts):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        out = model(**inputs)\n",
    "        # Apply ReLU and LogSumExp pooling (as used in SPLADE)\n",
    "        scores = F.relu(out.logits) * inputs.attention_mask.unsqueeze(-1)\n",
    "        scores = torch.log(1 + torch.sum(scores, dim=1))  # LogSumExp approx\n",
    "        return scores\n",
    "\n",
    "docs = [\n",
    "    \"Summarize the following article into a concise paragraph that highlights the key points, main arguments, and conclusions. Focus on preserving the factual accuracy and logical flow, and keep the summary within 100 words. Ensure technical or domain-specific terms are retained when relevant to context.\",\n",
    "    \"Generate Python code that implements the following functionality: a function that accepts a list of integers and returns a new list containing only the even numbers, sorted in ascending order. The code should be clean, readable, and follow PEP8 conventions. Avoid using external libraries unless necessary.\",\n",
    "    \"Generate 5 advanced-level interview questions for a candidate applying for a machine learning engineering role. The questions should test the candidate's understanding of ML theory, model optimization, production deployment, and performance evaluation. Avoid basic questions â€” target real-world expertise and problem-solving.\",\n",
    "    \"Translate the following technical paragraph from English to German. Maintain accuracy, terminology, and formatting, especially for code or formula-like elements. Ensure that the translation reflects native-level fluency and reads naturally in the target language. Avoid literal translations that distort the meaning.\",\n",
    "    \"Write a compelling product description for a new AI-powered note-taking app that automatically summarizes meetings, organizes tasks, and syncs with calendars. Highlight its key features, benefits, and how it saves time for professionals. The tone should be engaging, concise, and persuasive for tech-savvy users.\"\n",
    "]\n",
    "\n",
    "queries = [\"translation prompt\"]\n",
    "\n",
    "doc_vecs = splade_encode(docs)  # [num_docs x vocab_size]\n",
    "query_vecs = splade_encode(queries)\n",
    "\n",
    "# Dot-product scoring (sparse vector math)\n",
    "scores = torch.matmul(query_vecs, doc_vecs.T)\n",
    "topk = torch.topk(scores, k=2)\n",
    "\n",
    "print(\"Top results:\", [docs[i] for i in topk.indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results: ['Translate the following technical paragraph from English to German. Maintain accuracy, terminology, and formatting, especially for code or formula-like elements. Ensure that the translation reflects native-level fluency and reads naturally in the target language. Avoid literal translations that distort the meaning.', 'Summarize the following article into a concise paragraph that highlights the key points, main arguments, and conclusions. Focus on preserving the factual accuracy and logical flow, and keep the summary within 100 words. Ensure technical or domain-specific terms are retained when relevant to context.']\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99649e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
